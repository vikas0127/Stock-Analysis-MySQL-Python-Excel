{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "220c0e31-9233-4b44-95ee-0b9dcf617c08",
   "metadata": {},
   "source": [
    "## 1.Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9972b98c-b3ca-4ab2-bed0-fe2ba0e88336",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time \n",
    "import os\n",
    "from urllib.parse import quote_plus\n",
    "import yfinance as yf\n",
    "\n",
    "# Database Interaction\n",
    "import mysql.connector # Import the driver\n",
    "from sqlalchemy import create_engine, text # For pandas integration\n",
    "\n",
    "\n",
    "# import credentials from config file (if using)\n",
    "try:\n",
    "    from config import ALPHA_VANTAGE_API_KEY, MYSQL_HOST, MYSQL_USER, MYSQL_PASSWORD, MYSQL_DATABASE\n",
    "except ImportError:\n",
    "    print(\"Error: config.py not found or variables missing.\")\n",
    "    print(\"Please create config.py with your API Key and MySQL credentials.\")\n",
    "    # You could prompt the user here or set default placeholder values\n",
    "    APLHA_VANTAGE_API_KEY = 'YOUR_API_KEY_PLACEHOLDER'\n",
    "    MYSQL_HOST = 'localhost'\n",
    "    MYSQL_USER = 'root'\n",
    "    MYSQL_PASSWORD = 'password'\n",
    "    MYSQL_DATABASE = 'stock_market_db'\n",
    "\n",
    "\n",
    "# --- Project Configuration ---\n",
    "TICKERS = ['AAPL','MSFT','GOOGL'] # Stocks to fetch\n",
    "API_URL = 'https://www.alphavantage.co/query'\n",
    "FUNCTION = 'TIME_SERIES_DAILY_ADJUSTED'\n",
    "OUTPUT_SIZE = 'compact'  # 'compact' for 100 days, 'full' for more history. We're starting with compact for testing\n",
    "\n",
    "\n",
    "# Moving Average Windows\n",
    "MA_SHORT = 20 # Shorter window for compact data\n",
    "MA_LONG = 50 # Longer window\n",
    "\n",
    "\n",
    "# Output Configuration\n",
    "OUTPUT_DIR = 'output_mysql' # Directory to save plots and Excel file\n",
    "EXCEL_FILENAME = os.path.join(OUTPUT_DIR, 'stock_analyses_report_mysql.xlsx')\n",
    "PLOT_FILENAME_TEMPLATE = os.path.join(OUTPUT_DIR, '{}_price_MA_plot_mysql.png')\n",
    "\n",
    "# Ensure output directory exists\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "    print(f\"Created output directory: {OUTPUT_DIR}\")\n",
    "\n",
    "# For inline plots in Jupyter\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f1130f-8c93-427b-82f7-4feae6f6d573",
   "metadata": {},
   "source": [
    "## 2.Database Connection Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fd87edf-4da8-4d5a-a663-61f401a98ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to MySQL database!\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 2 Cleaned ---\n",
    "try:\n",
    "    # URL-encode the password if it contains special characters\n",
    "    encoded_password = quote_plus(MYSQL_PASSWORD)\n",
    "\n",
    "    # Build the connection string using the encoded password\n",
    "    connection_string = f\"mysql+mysqlconnector://{MYSQL_USER}:{encoded_password}@{MYSQL_HOST}/{MYSQL_DATABASE}\"\n",
    "\n",
    "    engine = create_engine(connection_string)\n",
    "\n",
    "    # Test connection\n",
    "    with engine.connect() as connection:\n",
    "        print(\"Successfully connected to MySQL database!\") # KEEP\n",
    "\n",
    "except NameError as ne: # Keep error handling\n",
    "    print(f\"Caught NameError: {ne}. Did you forget to run Cell 1 after restarting the kernel?\")\n",
    "    engine=None\n",
    "except ImportError: # Keep error handling\n",
    "    print(\"Error: mysql-connector-python or sqlalchemy not installed properly.\")\n",
    "    engine=None\n",
    "except Exception as e: # Keep error handling\n",
    "    print(f\"Error connecting to MySQL database: {e}\")\n",
    "    print(\"Please check your MySQL server status and credentials in config.py.\")\n",
    "    engine = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1589ab1a-df93-4346-b447-63f0d8292d7d",
   "metadata": {},
   "source": [
    "## 3.Create Database Table (if it doesn't exist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ab0bca4-f226-4f52-9c83-eab2925d17f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 'daily_stock_data' created successfully or already exists.\n"
     ]
    }
   ],
   "source": [
    "# --- Create Table Definition ---\n",
    "# Note: You can also create this table directly in MySQL Workbench first.\n",
    "# Using DECIMAL for prices is generally better for financial data than FLOAT/REAL.\n",
    "# VARCHAR is suitable for the symbol and date string.\n",
    "# Define the table name\n",
    "TABLE_NAME = 'daily_stock_data'\n",
    "\n",
    "create_table_sql = f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {TABLE_NAME} (\n",
    "    symbol VARCHAR(10) NOT NULL,\n",
    "    date DATE NOT NULL,        -- Use DATE type for dates in MySQL\n",
    "    open DECIMAL(10, 4),       -- Precision 10, 4 decimal places\n",
    "    high DECIMAL(10, 4),\n",
    "    low DECIMAL(10, 4),\n",
    "    close DECIMAL(10, 4),\n",
    "    adjusted_close DECIMAL(10, 4),\n",
    "    volume BIGINT,             -- Use BIGINT for potentially large volumes\n",
    "    PRIMARY KEY (symbol, date) -- Composite primary key prevents duplicates\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "# Execute the CREATE TABLE statement\n",
    "if engine:\n",
    "    try:\n",
    "        with engine.connect() as connection:\n",
    "            connection.execute(text(create_table_sql))\n",
    "            connection.commit() \n",
    "        print(f\"Table '{TABLE_NAME}' created successfully or already exists.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating table '{TABLE_NAME}': {e}\")\n",
    "else:\n",
    "    print(\"Skipping table creation due to connection error.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f148d5f-9e04-47cb-a02e-0fc5c9872b2f",
   "metadata": {},
   "source": [
    "## 4: Data Fetching Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d92d9ca8-0a68-4f7d-9360-15457e256103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 4 Cleaned ---\n",
    "import yfinance as yf # Make sure imports are here or in Cell 1\n",
    "import pandas as pd\n",
    "\n",
    "def fetch_stock_data_yf(symbol):\n",
    "    print(f\"Fetching data for {symbol} from Yahoo Finance...\") # KEEP\n",
    "    try:\n",
    "        ticker = yf.Ticker(symbol)\n",
    "        df = ticker.history(period=\"5y\")\n",
    "\n",
    "        if df.empty:\n",
    "            print(f\"No data returned for {symbol} from yfinance.\") # KEEP\n",
    "            return None\n",
    "\n",
    "        # --- Column renaming logic as developed ---\n",
    "        df.rename(columns={\n",
    "            'Open': 'open', 'High': 'high', 'Low': 'low',\n",
    "            'Close': 'close', 'Volume': 'volume'\n",
    "        }, inplace=True)\n",
    "        if 'Adj Close' in df.columns:\n",
    "             df.rename(columns={'Adj Close': 'adjusted_close'}, inplace=True)\n",
    "             cols_to_keep = ['open', 'high', 'low', 'close', 'adjusted_close', 'volume']\n",
    "        else:\n",
    "             df['adjusted_close'] = df['close']\n",
    "             cols_to_keep = ['open', 'high', 'low', 'close', 'adjusted_close', 'volume']\n",
    "        df = df[cols_to_keep]\n",
    "        # --- End renaming logic ---\n",
    "\n",
    "        df['symbol'] = symbol\n",
    "        df.reset_index(inplace=True)\n",
    "        df.rename(columns={'Date': 'date'}, inplace=True)\n",
    "        df['date'] = pd.to_datetime(df['date']).dt.date\n",
    "\n",
    "        print(f\"Successfully fetched {len(df)} data points for {symbol} using yfinance\") # KEEP\n",
    "        return df[['symbol', 'date', 'open', 'high', 'low', 'close', 'adjusted_close', 'volume']]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred fetching {symbol} using yfinance: {e}\") # KEEP\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e886b14-2685-42f6-b972-e4d274523a8c",
   "metadata": {},
   "source": [
    "## 5: Data Storage Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d00a16d0-c1bd-45c9-9670-32a751158f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_mysql(df, table_name, db_engine):\n",
    "    \"\"\"Saves the DataFrame to the MySQL database, handling duplicates via INSERT IGNORE.\"\"\"\n",
    "    if df is None or df.empty or db_engine is None:\n",
    "        print(\"No data/engine available to save.\")\n",
    "        return 0 # Return count of rows added\n",
    "\n",
    "    symbol = df['symbol'].iloc[0] if not df.empty else 'Unknown'\n",
    "    rows_added = 0\n",
    "\n",
    "    # Use pandas `to_sql` with a custom method for handling duplicates ('INSERT IGNORE')\n",
    "    # A more robust way than relying solely on `if_exists='append'` with primary key errors.\n",
    "    # Need to write raw SQL or use a temp table approach for complex upserts.\n",
    "    # Simplest here: try appending and let the primary key handle it (will log errors).\n",
    "    # OR: fetch existing dates and filter df before inserting. Let's try the PK approach first.\n",
    "\n",
    "    try:\n",
    "        # 'append' will try to insert all rows. The PRIMARY KEY constraint\n",
    "        # in MySQL will cause errors for duplicates, which pandas might raise.\n",
    "        df.to_sql(name=table_name, con=db_engine, if_exists='append', index=False)\n",
    "        # Note: `to_sql` with `append` doesn't return the number of rows actually inserted\n",
    "        # if some were duplicates. We assume success if no exception is raised.\n",
    "        # A more accurate count would require checking before/after counts or complex SQL.\n",
    "        print(f\"Attempted to save/append data for {symbol} to table '{table_name}'. Check MySQL logs for duplicate key errors if any.\")\n",
    "        # Simplistic row count - assumes all were potentially new\n",
    "        rows_added = len(df)\n",
    "\n",
    "    # Catch the specific MySQL IntegrityError if possible (might be wrapped by SQLAlchemy)\n",
    "    except Exception as e: # Catching a broader exception for now\n",
    "        if 'Duplicate entry' in str(e):\n",
    "             print(f\"Duplicate key error encountered for {symbol}. Some rows may not have been inserted.\")\n",
    "             # You could implement logic here to filter out existing dates and retry insertion.\n",
    "        else:\n",
    "             print(f\"Database error saving {symbol}: {e}\")\n",
    "    return rows_added # Return the attempted number of rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f903691f-585d-486b-8822-fab7c16187e8",
   "metadata": {},
   "source": [
    "##  6: Main Fetching and Storing Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46865f76-e80a-4c96-8fd0-40d8cc09bd3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing AAPL ---\n",
      "Fetching data for AAPL from Yahoo Finance...\n",
      "Successfully fetched 1257 data points for AAPL using yfinance\n",
      "Duplicate key error encountered for AAPL. Some rows may not have been inserted.\n",
      "Pausing for 15 seconds...\n",
      "\n",
      "--- Processing MSFT ---\n",
      "Fetching data for MSFT from Yahoo Finance...\n",
      "Successfully fetched 1257 data points for MSFT using yfinance\n",
      "Duplicate key error encountered for MSFT. Some rows may not have been inserted.\n",
      "Pausing for 15 seconds...\n",
      "\n",
      "--- Processing GOOGL ---\n",
      "Fetching data for GOOGL from Yahoo Finance...\n",
      "Successfully fetched 1257 data points for GOOGL using yfinance\n",
      "Duplicate key error encountered for GOOGL. Some rows may not have been inserted.\n",
      "Pausing for 15 seconds...\n",
      "\n",
      "Data fetching and storing process completed. Attempted to add 0 rows (duplicates ignored by DB).\n"
     ]
    }
   ],
   "source": [
    "# --- Fetch and Store Data for All Tickers ---\n",
    "total_rows_added = 0\n",
    "if engine: # Only proceed if the database connection is okay\n",
    "    for ticker in TICKERS:\n",
    "        print(f\"\\n--- Processing {ticker} ---\")\n",
    "        stock_df = fetch_stock_data_yf(ticker)\n",
    "\n",
    "        if stock_df is not None:\n",
    "            rows = save_to_mysql(stock_df, TABLE_NAME, engine)\n",
    "            total_rows_added += rows\n",
    "            # You can verify insertion by querying the table in MySQL Workbench\n",
    "            # SELECT * FROM daily_stock_data WHERE symbol = 'AAPL' ORDER BY date DESC LIMIT 5;\n",
    "        else:\n",
    "             print(f\"Skipping database save for {ticker} due to fetch error.\")\n",
    "\n",
    "        # IMPORTANT: Respect Alpha Vantage free tier limits (e.g., 5 calls per minute)\n",
    "        print(\"Pausing for 15 seconds...\")\n",
    "        time.sleep(15)\n",
    "\n",
    "    print(f\"\\nData fetching and storing process completed. Attempted to add {total_rows_added} rows (duplicates ignored by DB).\")\n",
    "else:\n",
    "    print(\"Database connection failed. Cannot fetch or store data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae35a65-4349-4a40-8a62-a217b3768ca2",
   "metadata": {},
   "source": [
    "## 7: Data Loading Function (from MySQL))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321f03d1-9f87-4fa7-acfd-3f93e0584e7e",
   "metadata": {},
   "source": [
    "def load_data_from_mysql(symbol, table_name, db_engine):\n",
    "    \"\"\"Loads stock data for a specific symbol from the MySQL database.\"\"\"\n",
    "    if db_engine is None:\n",
    "        print(\"Database engine not available.\")\n",
    "        return None\n",
    "\n",
    "    print(f\"\\nLoading data for {symbol} from MySQL...\")\n",
    "    try:\n",
    "        # Use pandas read_sql_query for safety and convenience\n",
    "        query = f\"SELECT * FROM {table_name} WHERE symbol = %s ORDER BY date ASC\"\n",
    "        # Use %s placeholder for mysqlconnector, SQLAlchemy handles parameterization\n",
    "        df = pd.read_sql_query(query, db_engine, params=(symbol,))\n",
    "\n",
    "        if df.empty:\n",
    "            print(f\"No data found in database for symbol: {symbol}\")\n",
    "            return None\n",
    "\n",
    "        # Convert 'date' column to datetime objects (if not already) and set as index\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        df.set_index('date', inplace=True)\n",
    "\n",
    "        # Convert decimal columns back to float for analysis if needed (or keep as object/Decimal)\n",
    "        # Pandas usually reads them as 'object' type representing Decimal. Convert to float for plotting/calculations.\n",
    "        for col in ['open', 'high', 'low', 'close', 'adjusted_close']:\n",
    "             if col in df.columns:\n",
    "                 df[col] = pd.to_numeric(df[col]) # Handles Decimal conversion\n",
    "\n",
    "        df['volume'] = pd.to_numeric(df['volume'])\n",
    "\n",
    "        print(f\"Loaded {len(df)} data points for {symbol} from database.\")\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data for {symbol} from MySQL: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e06f540-3c93-4b90-8256-881f6520f307",
   "metadata": {},
   "source": [
    "## 8: Analysis Function (Moving Averages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0fb3b216-1c14-4ed5-ab8d-0bacc4bcdac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_moving_averages_pd(df, short_window=MA_SHORT, long_window=MA_LONG):\n",
    "    \"\"\"Calculates short-term and long-term moving averages using Pandas.\"\"\"\n",
    "    if df is None or 'adjusted_close' not in df.columns:\n",
    "        print(\"DataFrame is None or missing 'adjusted_close' column.\")\n",
    "        return None\n",
    "\n",
    "    # Ensure the column is numeric\n",
    "    df['adjusted_close'] = pd.to_numeric(df['adjusted_close'])\n",
    "\n",
    "    df[f'MA_{short_window}'] = df['adjusted_close'].rolling(window=short_window).mean()\n",
    "    df[f'MA_{long_window}'] = df['adjusted_close'].rolling(window=long_window).mean()\n",
    "    print(f\"Calculated {short_window}-day and {long_window}-day Moving Averages.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d22583b-c356-4f64-b536-da0c98863883",
   "metadata": {},
   "source": [
    "## 9: Plotting Function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5bb0db19-5293-4099-afe1-8c6af13a30b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stock_data(df, symbol):\n",
    "    \"\"\"Generates and saves a plot of adjusted close price and moving averages.\"\"\"\n",
    "    if df is None:\n",
    "        print(f\"Cannot plot data for {symbol}, DataFrame is None.\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(12, 6)) # Adjusted size slightly for notebook view\n",
    "    plt.plot(df.index, df['adjusted_close'], label='Adjusted Close', alpha=0.9)\n",
    "\n",
    "    ma_short_col = f'MA_{MA_SHORT}'\n",
    "    ma_long_col = f'MA_{MA_LONG}'\n",
    "\n",
    "    if ma_short_col in df.columns and not df[ma_short_col].isnull().all():\n",
    "      plt.plot(df.index, df[ma_short_col], label=f'{MA_SHORT}-Day MA', linestyle='--')\n",
    "    if ma_long_col in df.columns and not df[ma_long_col].isnull().all():\n",
    "      plt.plot(df.index, df[ma_long_col], label=f'{MA_LONG}-Day MA', linestyle=':')\n",
    "\n",
    "    plt.title(f'{symbol} - Adjusted Close Price and Moving Averages (MySQL Data)')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Price (USD)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout() # Adjust layout\n",
    "\n",
    "    # Save the plot\n",
    "    plot_filename = PLOT_FILENAME_TEMPLATE.format(symbol)\n",
    "    try:\n",
    "        plt.savefig(plot_filename)\n",
    "        print(f\"Plot saved to {plot_filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving plot {plot_filename}: {e}\")\n",
    "\n",
    "    plt.show() # Display the plot inline in Jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dccedea-c445-4f80-88f0-40d567d419d9",
   "metadata": {},
   "source": [
    "## 10: Excel Export Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cd8c3fe5-4b9c-4b64-b753-6b1da28e3799",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_analysis_to_excel(data_dict, filename):\n",
    "    \"\"\"Exports the analyzed dataframes to separate sheets in an Excel file.\"\"\"\n",
    "    if not data_dict:\n",
    "        print(\"No data to export.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nExporting analysis to Excel: {filename}\")\n",
    "    try:\n",
    "        with pd.ExcelWriter(filename, engine='openpyxl') as writer:\n",
    "            for symbol, df in data_dict.items():\n",
    "                if df is not None:\n",
    "                    # Select relevant columns, reset index to include date\n",
    "                    cols_to_export = ['adjusted_close', f'MA_{MA_SHORT}', f'MA_{MA_LONG}']\n",
    "                    df_to_export = df[cols_to_export].copy()\n",
    "                    df_to_export.reset_index(inplace=True) # Date becomes a column\n",
    "                    # Format date for Excel readability\n",
    "                    df_to_export['date'] = df_to_export['date'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "                    # Handle potential missing MA columns if windows were too large for data\n",
    "                    for col in cols_to_export:\n",
    "                        if col not in df_to_export.columns:\n",
    "                            df_to_export[col] = None # Add empty column if missing\n",
    "\n",
    "                    df_to_export.to_excel(writer, sheet_name=symbol, index=False)\n",
    "            print(f\"Analysis results exported successfully to {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error exporting data to Excel: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74bd0f1-5e1e-448e-9951-6a9e2888f309",
   "metadata": {},
   "source": [
    "## 11: Main Analysis Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8339b1af-84fc-4934-aaed-55a50ea7bec7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_data_from_mysql' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m engine: \u001b[38;5;66;03m# Only proceed if database connection is okay\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ticker \u001b[38;5;129;01min\u001b[39;00m TICKERS:\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;66;03m# 1. Load data using SQL (via Python/SQLAlchemy)\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m         stock_df \u001b[38;5;241m=\u001b[39m load_data_from_mysql(ticker, TABLE_NAME, engine)\n\u001b[0;32m      9\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m stock_df \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     10\u001b[0m             \u001b[38;5;66;03m# 2. Perform Analysis (Python/Pandas)\u001b[39;00m\n\u001b[0;32m     11\u001b[0m             stock_df \u001b[38;5;241m=\u001b[39m calculate_moving_averages_pd(stock_df)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'load_data_from_mysql' is not defined"
     ]
    }
   ],
   "source": [
    "# --- Run the Analysis and Reporting ---\n",
    "analyzed_data_mysql = {} # Dictionary to hold dataframes for Excel export\n",
    "\n",
    "if engine: # Only proceed if database connection is okay\n",
    "    for ticker in TICKERS:\n",
    "        # 1. Load data using SQL (via Python/SQLAlchemy)\n",
    "        stock_df = load_data_from_mysql(ticker, TABLE_NAME, engine)\n",
    "\n",
    "        if stock_df is not None:\n",
    "            # 2. Perform Analysis (Python/Pandas)\n",
    "            stock_df = calculate_moving_averages_pd(stock_df)\n",
    "\n",
    "            # 3. Visualize (Python/Matplotlib - displayed inline)\n",
    "            plot_stock_data(stock_df, ticker)\n",
    "\n",
    "            # Store for Excel export\n",
    "            analyzed_data_mysql[ticker] = stock_df\n",
    "        else:\n",
    "            print(f\"Skipping analysis and plotting for {ticker} due to loading error.\")\n",
    "\n",
    "    # 4. Export to Excel (Python/Pandas)\n",
    "    if analyzed_data_mysql:\n",
    "        export_analysis_to_excel(analyzed_data_mysql, EXCEL_FILENAME)\n",
    "    else:\n",
    "        print(\"No data was analyzed, skipping Excel export.\")\n",
    "\n",
    "    print(\"\\nAnalysis and reporting process completed.\")\n",
    "\n",
    "else:\n",
    "    print(\"Database connection failed. Cannot run analysis.\")\n",
    "\n",
    "# Optional: Close the engine when completely done (though usually not necessary for scripts)\n",
    "# if engine:\n",
    "#    engine.dispose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b43695-4a7c-47d3-a934-9edc3ced37fe",
   "metadata": {},
   "source": [
    "## 12: Using MySQL Workbench"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bda22a-d62d-4e53-a7d9-3e14148fb299",
   "metadata": {},
   "source": [
    "### Using MySQL Workbench\n",
    "\n",
    "Now that the script has run:\n",
    "\n",
    "1.  **Open MySQL Workbench** and connect to your server.\n",
    "2.  **Navigate** to the `stock_market_db` schema (database).\n",
    "3.  **Expand Tables:** You should see the `daily_stock_data` table.\n",
    "4.  **Inspect Data:** Right-click on `daily_stock_data` and choose \"Select Rows - Limit 1000\". You'll see the raw data stored by the Python script.\n",
    "5.  **Run Queries:** You can run your own SQL queries directly in Workbench to explore the data further, for example:\n",
    "    ```sql\n",
    "    -- Get the latest entry for each stock\n",
    "    SELECT symbol, MAX(date) AS latest_date\n",
    "    FROM daily_stock_data\n",
    "    GROUP BY symbol;\n",
    "\n",
    "    -- Count rows per stock\n",
    "    SELECT symbol, COUNT(*) AS row_count\n",
    "    FROM daily_stock_data\n",
    "    GROUP BY symbol;\n",
    "\n",
    "    -- Find days with large price swings for AAPL (e.g., high-low > 5% of open)\n",
    "    SELECT date, open, high, low, close\n",
    "    FROM daily_stock_data\n",
    "    WHERE symbol = 'AAPL' AND (high - low) > (open * 0.05)\n",
    "    ORDER BY date DESC;\n",
    "    ```\n",
    "This demonstrates your ability to use SQL *independently* to query the database created and populated by your Python process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cb9e90-8261-40d6-ba03-e03b5839928e",
   "metadata": {},
   "source": [
    "### Interpretation and Next Steps\n",
    "\n",
    "*   **Moving Averages:** The plots show the stock's adjusted closing price along with its short-term (e.g., 20-day) and long-term (e.g., 50-day) moving averages.\n",
    "    *   When the price is above the MAs, it can indicate an uptrend.\n",
    "    *   When the short-term MA crosses above the long-term MA (a \"golden cross\"), it's often seen as a bullish signal.\n",
    "    *   When the short-term MA crosses below the long-term MA (a \"death cross\"), it's often seen as a bearish signal.\n",
    "*   **Excel Report:** The generated `stock_analysis_report_mysql.xlsx` file contains the raw adjusted close and calculated MAs for each stock on separate sheets. This file could be shared or used for further ad-hoc analysis or dashboarding in Excel itself.\n",
    "*   **Potential Improvements:**\n",
    "    *   Add more technical indicators (RSI, MACD).\n",
    "    *   Calculate daily/weekly returns and volatility.\n",
    "    *   Incorporate volume analysis alongside price.\n",
    "    *   Implement error handling for API rate limits more gracefully.\n",
    "    *   Build an interactive dashboard (e.g., using Streamlit or Dash) instead of static plots/Excel.\n",
    "    *   Optimize database insertion for very large datasets (e.g., bulk inserts, handling duplicates more efficiently)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ca195c-7460-4db7-bb35-05e28b8f4911",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e81c687-8db1-4dde-8c2d-8614728d38de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e287e32a-74d9-44a9-9aab-1f4e105ad355",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
